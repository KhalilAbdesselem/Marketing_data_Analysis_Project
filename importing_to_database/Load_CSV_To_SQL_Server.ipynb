{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3f04557",
   "metadata": {},
   "source": [
    "# Loading CSV Data into SQL Server\n",
    "This notebook demonstrates how to load data from CSV files into a SQL Server database using Python, `pyodbc`, and `pandas`. The process includes establishing a connection, creating tables, and inserting data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfc4334",
   "metadata": {},
   "source": [
    "## Step 1: Importing Required Libraries\n",
    "Import necessary libraries for connecting to SQL Server (`pyodbc`), handling file operations (`os`), and working with data (`pandas`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097394c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91aae498",
   "metadata": {},
   "source": [
    "## Step 2: Setting Up the Connection String\n",
    "Define the connection string parameters to connect to the SQL Server database. Make sure to replace the server and database names with your actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70f5f0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n"
     ]
    }
   ],
   "source": [
    "server = 'DESKTOP-GJR311M\\\\PROJECT_BI_2BA2'  # Your server name\n",
    "database = 'Youtube_DB'  # Replace with your actual database name\n",
    "\n",
    "# Use Trusted_Connection=yes for Windows Authentication\n",
    "conn_str = f'DRIVER={{SQL Server}};SERVER={server};DATABASE={database};Trusted_Connection=yes;'\n",
    "# Test the connection\n",
    "try:\n",
    "    conn = pyodbc.connect(conn_str)\n",
    "    print(\"Connection successful!\")\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bbdfe4",
   "metadata": {},
   "source": [
    "## Step 3: Loading channels and their performance CSV Data into SQL Server\n",
    "This step automates the process of loading data from multiple CSV files into a SQL Server database:\n",
    "\n",
    "1. **Connect to SQL Server**: Establish a connection using the provided connection string and create a cursor for executing SQL commands.\n",
    "2. **Folder Path Setup**: Specify the folder containing the CSV files to be imported.\n",
    "3. **Loop Through CSV Files**: Iterate over each CSV file in the specified folder:\n",
    "   - **Load the CSV into a DataFrame**: Read the file into a pandas DataFrame.\n",
    "   - **Create Table in SQL Server**: Generate a `CREATE TABLE` SQL query based on the DataFrame columns, using `NVARCHAR(MAX)` as the data type for each column. The table name is derived from the CSV file name.\n",
    "   - **Insert Data into the Table**: Insert the data row by row from the DataFrame into the newly created SQL table.\n",
    "4. **Commit Changes**: Commit the transaction after all rows from the CSV have been inserted to ensure data is saved.\n",
    "5. **Close the Connection**: Close the SQL Server connection after processing all files.\n",
    "\n",
    "This approach ensures that each CSV file is loaded into a separate table within the database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e0f6185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from all_performance_data_Australia.csv inserted into table [all_performance_data_Australia].\n",
      "Data from all_performance_data_Canada.csv inserted into table [all_performance_data_Canada].\n",
      "Data from all_performance_data_UK.csv inserted into table [all_performance_data_UK].\n",
      "Data from all_performance_data_USA.csv inserted into table [all_performance_data_USA].\n",
      "Data from CChannels_Australia.csv inserted into table [CChannels_Australia].\n",
      "Data from Channels_Canada.csv inserted into table [Channels_Canada].\n",
      "Data from Channels_UK.csv inserted into table [Channels_UK].\n",
      "Data from Channels_USA.csv inserted into table [Channels_USA].\n"
     ]
    }
   ],
   "source": [
    "# Connect to SQL Server\n",
    "conn = pyodbc.connect(conn_str)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Folder path containing CSV files\n",
    "folder_path = r'C:\\Users\\khalil\\Desktop\\Youtube_Data'\n",
    "\n",
    "# Loop through each CSV file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "        # Full file path\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Load the CSV file into a pandas DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Extract the table name (same as the file name without the .csv extension)\n",
    "        table_name = os.path.splitext(file_name)[0]\n",
    "        \n",
    "        # Prepare the CREATE TABLE SQL query\n",
    "        create_table_query = f\"CREATE TABLE [{table_name}] (\"\n",
    "        for col in df.columns:\n",
    "            # Enclose column names in square brackets and use NVARCHAR(MAX)\n",
    "            create_table_query += f\"[{col}] NVARCHAR(MAX),\"\n",
    "        create_table_query = create_table_query.rstrip(\",\") + \")\"\n",
    "        \n",
    "        # Execute the create table query (this creates a table if it doesn't exist)\n",
    "        cursor.execute(create_table_query)\n",
    "        conn.commit()\n",
    "        \n",
    "        # Insert DataFrame into SQL Server table row by row\n",
    "        for index, row in df.iterrows():\n",
    "            insert_query = f\"INSERT INTO [{table_name}] VALUES ({','.join(['?' for _ in row])})\"\n",
    "            cursor.execute(insert_query, tuple(row))\n",
    "        \n",
    "        # Commit after inserting all rows for this CSV\n",
    "        conn.commit()\n",
    "\n",
    "        print(f\"Data from {file_name} inserted into table [{table_name}].\")\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e941ab2",
   "metadata": {},
   "source": [
    "## Step 4: Loading YouTube Trending Data into SQL Server\n",
    "This step involves loading the YouTube trending data from a CSV file into a SQL Server database:\n",
    "\n",
    "1. **Connect to SQL Server**: Establish a connection using the provided connection string and create a cursor for executing SQL commands.\n",
    "2. **Load CSV Data into a DataFrame**: Read the `youtube_trending_data.csv` file into a pandas DataFrame.\n",
    "3. **Table Preparation**:\n",
    "   - **Define the Table Name**: Set the table name as 'trending'.\n",
    "   - **Drop Existing Table**: If a table named 'trending' already exists in the database, drop it to avoid conflicts.\n",
    "   - **Create a New Table**: Dynamically generate a `CREATE TABLE` SQL query based on the DataFrame columns, using `NVARCHAR(MAX)` as the data type for each column.\n",
    "4. **Data Cleaning**:\n",
    "   - **Define a Cleaning Function**: Use the `clean_value` function to handle invalid or infinite values by converting them to `None`.\n",
    "5. **Insert Cleaned Data into the Table**:\n",
    "   - **Iterate Over Rows**: Clean each row using the `clean_value` function and insert the data into the 'trending' table row by row.\n",
    "6. **Commit Changes**: Commit the transaction after all rows have been inserted to ensure the data is saved.\n",
    "7. **Close the Connection**: Close the SQL Server connection after completing the data insertion.\n",
    "\n",
    "This process ensures that the YouTube trending data is loaded into the database in a clean and structured format for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4baac965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'trending' created successfully.\n",
      "Data from C:\\Users\\khalil\\marketing_data_Analytics_Project\\youtube_trending_data.csv inserted into table [trending].\n"
     ]
    }
   ],
   "source": [
    "# Connect to SQL Server\n",
    "conn = pyodbc.connect(conn_str)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Full path to the CSV file\n",
    "file_path = r'C:\\Users\\khalil\\marketing_data_Analytics_Project\\youtube_trending_data.csv'\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Extract the table name (for this case, we'll use 'trending')\n",
    "table_name = 'trending'\n",
    "\n",
    "# Drop the table if it already exists (this avoids the \"table already exists\" error)\n",
    "drop_table_query = f\"IF OBJECT_ID('{table_name}', 'U') IS NOT NULL DROP TABLE [{table_name}]\"\n",
    "cursor.execute(drop_table_query)\n",
    "conn.commit()\n",
    "\n",
    "# Prepare the CREATE TABLE SQL query dynamically based on the DataFrame columns\n",
    "create_table_query = f\"CREATE TABLE [{table_name}] (\"\n",
    "for col in df.columns:\n",
    "    # Enclose column names in square brackets and use NVARCHAR(MAX) as the column type\n",
    "    create_table_query += f\"[{col}] NVARCHAR(MAX),\"\n",
    "create_table_query = create_table_query.rstrip(\",\") + \")\"\n",
    "\n",
    "# Execute the create table query (creates the table based on the CSV columns)\n",
    "try:\n",
    "    cursor.execute(create_table_query)\n",
    "    conn.commit()\n",
    "    print(f\"Table '{table_name}' created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Table creation failed: {e}\")\n",
    "\n",
    "# Clean the DataFrame before inserting it into SQL Server\n",
    "def clean_value(value):\n",
    "    \"\"\"Convert values to None if they cannot be converted to SQL-appropriate data types.\"\"\"\n",
    "    try:\n",
    "        # Convert infinite values or invalid values to None\n",
    "        if pd.isna(value) or value == float('inf') or value == float('-inf'):\n",
    "            return None\n",
    "        else:\n",
    "            return str(value)  # Convert to string to avoid SQL data type issues\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Insert DataFrame into SQL Server table row by row\n",
    "for index, row in df.iterrows():\n",
    "    # Clean each row by applying the clean_value function\n",
    "    cleaned_row = [clean_value(val) for val in row]\n",
    "\n",
    "    # Dynamically generate the placeholders for the insert query\n",
    "    insert_query = f\"INSERT INTO [{table_name}] VALUES ({','.join(['?' for _ in cleaned_row])})\"\n",
    "    \n",
    "    # Execute the insert query, inserting cleaned data\n",
    "    cursor.execute(insert_query, tuple(cleaned_row))\n",
    "\n",
    "# Commit after inserting all rows\n",
    "conn.commit()\n",
    "\n",
    "print(f\"Data from {file_path} inserted into table [{table_name}].\")\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b965d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
